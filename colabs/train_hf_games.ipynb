{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdEelqwPVTrxPj7UHuk4Yk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbiish/huggingface_courses/blob/main/colabs/train_hf_games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code from the Unit 1 ðŸ¤— RL Colab HereðŸ‘‰[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)"
      ],
      "metadata": {
        "id": "oOqsTWma2IF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements from ðŸ¤—RL Unit Colab"
      ],
      "metadata": {
        "id": "h8sDeBbKRP0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_e7C4x5zly0"
      },
      "outputs": [],
      "source": [
        "!apt install swig cmake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
      ],
      "metadata": {
        "id": "MneVLe-x0BbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "90P3iQpm0DVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rconnect runtime and continue running cells after this this process kill"
      ],
      "metadata": {
        "id": "UJHfHoUrRmTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "wScdvPox0Fpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up environment (virtual display and ðŸ¤— setup)"
      ],
      "metadata": {
        "id": "XPekcqklR62L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "gKIR8C6s0ICJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "y9G9LpNB0J8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up game environment  \n",
        "\n",
        "TODO:\n",
        "- [ ] options for changing games"
      ],
      "metadata": {
        "id": "pbdq_2i9SO68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# First, we create our environment called LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Then we reset this environment\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Do this action in the environment and get\n",
        "  # next_state, reward, terminated, truncated and info\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  \n",
        "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
        "  if terminated or truncated:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5CoOSKWL0LmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "_yasowNO0OWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "xraiQkMI0QeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ],
      "metadata": {
        "id": "gsqC9jy00To5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change model training params here  \n",
        "\n",
        "TODO:\n",
        "- [ ] add colab params for variables"
      ],
      "metadata": {
        "id": "bRoudCnnSijP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We added some parameters to accelerate the training\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = env,\n",
        "    n_steps = 1024,\n",
        "    batch_size = 64,\n",
        "    n_epochs = 4,\n",
        "    gamma = 0.999,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = 0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "0nZMzyZ30Vej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train it for 1,000,000 timesteps\n",
        "model.learn(total_timesteps=1000000)\n",
        "# Save the model\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "CiFwiPeZ0aAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation"
      ],
      "metadata": {
        "id": "IcAjrJTKTdDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Gt\n",
        "from torch import gt\n",
        "\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
        "\n",
        "while mean_reward <= 288.95:\n",
        "  eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "  mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "  print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "NsdznkRP0cN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Send model to ðŸ¤—"
      ],
      "metadata": {
        "id": "r5Xw9QaJ0g_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "IWj12cV30e2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from huggingface_sb3 import package_to_hub\n",
        "\n",
        "# PLACE the variables you've just defined two cells above\n",
        "# Define the name of the environment\n",
        "env_id = \"LunarLander-v2\"\n",
        "\n",
        "# TODO: Define the model architecture we used\n",
        "model_architecture = \"PPO\"\n",
        "\n",
        "## Define a repo_id\n",
        "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
        "## CHANGE WITH YOUR REPO ID\n",
        "repo_id = \"nbiish/LearnHF-LunarLander-v2\" # Change with your repo id, you can't push with mine ðŸ˜„\n",
        "\n",
        "## Define the commit message\n",
        "commit_message = \"The key is a while loop on the mean_reward variable when evaluating your agentðŸ¥°ðŸ“š\"\n",
        "\n",
        "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
        "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "\n",
        "# PLACE the package_to_hub function you've just filled here\n",
        "package_to_hub(model=model, # Our trained model\n",
        "               model_name=model_name, # The name of our trained model \n",
        "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
        "               env_id=env_id, # Name of the environment\n",
        "               eval_env=eval_env, # Evaluation Environment\n",
        "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
        "               commit_message=commit_message)"
      ],
      "metadata": {
        "id": "hctY9-Th0tb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load another model\n",
        "\n",
        "TODO:\n",
        "- [ ] add colab params for variables"
      ],
      "metadata": {
        "id": "his3bVRx0xib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install for older molders made using \"Gym\" vs \"Gymnasium\"(newer verion)\n",
        "!pip install shimmy"
      ],
      "metadata": {
        "id": "y9X3rpdx0u2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_sb3 import load_from_hub\n",
        "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
        "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
        "\n",
        "\n",
        "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
        "# But Python 3.6, 3.7 use protocol 4\n",
        "# In order to get compatibility we need to:\n",
        "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
        "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
        "custom_objects = {\n",
        "            \"learning_rate\": 0.0,\n",
        "            \"lr_schedule\": lambda _: 0.0,\n",
        "            \"clip_range\": lambda _: 0.0,\n",
        "}\n",
        "\n",
        "checkpoint = load_from_hub(repo_id, filename)\n",
        "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
      ],
      "metadata": {
        "id": "uM5Hmb5Z1DWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "-WyssoYN1FpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}